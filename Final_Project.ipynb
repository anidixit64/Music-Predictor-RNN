{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtR3TmGjd8VdyDp+MPRblM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anidixit64/Music-Predictor-RNN/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I used an RNN because it tends to work better with linguistic processing and language, but after my attempt to use an LSTM\n",
        "went a little sideways I decieded to use a regular RNN model using the PyTorch documentation. It seems like incorporating\n",
        "it into classification tasks would also be a little easier than using Kanerva memory, and I feel like I can use it practically\n",
        "better than N-grams.\n",
        "\n",
        "I measured accuracy with a simple #correct/total # predicted because with the model I was using it made the most sense to have\n",
        "a straightforward way of keeping track of the changing accuracy rates. I also understood it the best so I kept it to ensure the\n",
        "rest of my code wasn't too complicated. Since my arrays for both embedded labels and text sequences for the lyrics are the same\n",
        "size, I can index them in parallel, so I just kept a running counter of 'correct' guesses by comparing both the training and\n",
        "testing datasets with the predicted value arrays and used that to calculate a percentage accuracy for both the training and testing\n",
        "functions. I think that this method overall, while simple, gives me more leniency since individual line mistakes for specific lyrics\n",
        "or sequences can be handwaved in the larger context of the accuracy, and that small mistakes have a smaller effect on the actual\n",
        "testing accuracy.\n",
        "\n",
        "With the error, I think the style and substance of the authors the errors fell on explains why the data might have gone off. For\n",
        "example, I guesses Margaret Atwood for Mary Oliver, which makes sense to me since both are poets or at least poetic authors. Similarly,\n",
        "bands like camp and the wallows, which tend to make indie or folk music are replaced in the guesses by artists like John Lennon, who\n",
        "make music with similar themes and musical styles. In other words, I think that the style of music or genre shines through the lyrics\n",
        "(i.e pop, folk, literary fiction, rock, etc.) and that the RNN predicted the genre of the work more often than the author themselves. If\n",
        "I had more time, I think it would have been interesting to try and build another dataset with 'genre' classifications to test this.\n",
        "'''\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open('trainfile-2.json', 'r') as train_file:\n",
        "    train_data = json.load(train_file)\n",
        "\n",
        "with open('testfile-2.json', 'r') as test_file:\n",
        "    test_data = json.load(test_file)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n",
        "\n",
        "all_training_lyrics = [i[0].lower() for i in train_data]\n",
        "all_training_labels = [i[1].lower() for i in train_data]\n",
        "all_testing_lyrics = [i[0].lower() for i in test_data]\n",
        "all_testing_labels = [i[1].lower() for i in test_data]\n",
        "\n",
        "print('\\nALL DATA')\n",
        "print(f'Training Data: {all_training_lyrics}')\n",
        "print(f'Training Labels: {all_training_labels}')\n",
        "print(f'Testing Data: {all_testing_lyrics}')\n",
        "print(f'Testing Labels: {all_testing_labels}')\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_training_lyrics)\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "print('\\nTOKENIZER VOCAB LIST')\n",
        "print(f'{vocab_size}: {tokenizer.word_index}')\n",
        "\n",
        "training_sequences = []\n",
        "training_sequences += tokenizer.texts_to_sequences(i for i in all_training_lyrics)\n",
        "testing_sequences = []\n",
        "testing_sequences += tokenizer.texts_to_sequences(i for i in all_testing_lyrics)\n",
        "print('\\nTRAIN SEQUENCES')\n",
        "print(training_sequences)\n",
        "print(testing_sequences)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_training_labels)\n",
        "num_artists = len(label_encoder.classes_)\n",
        "print('\\nENCODINGS')\n",
        "print(f'NUM ARTISTS: {num_artists}')\n",
        "\n",
        "encoded_training_labels = label_encoder.transform(all_training_labels)\n",
        "encoded_testing_labels = label_encoder.transform(all_testing_labels)\n",
        "print('\\nENCODED LABELS')\n",
        "print(encoded_training_labels)\n",
        "print(encoded_testing_labels)"
      ],
      "metadata": {
        "id": "bie8YT6cR12v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "input_size = vocab_size + 1\n",
        "hidden_size = 200\n",
        "output_size = num_artists\n",
        "learning_rate = 0.001\n",
        "num_epochs = 15\n",
        "batch_size = 64\n",
        "\n",
        "model = RNN(input_size, hidden_size, output_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "X_train = torch.LongTensor(pad_sequences(training_sequences))\n",
        "Y_train = torch.LongTensor(encoded_training_labels)\n",
        "X_test = torch.LongTensor(pad_sequences(testing_sequences))\n",
        "Y_test = torch.LongTensor(encoded_testing_labels)\n",
        "print('\\nX_Y TRAIN AND TEST')\n",
        "print(f'X TRAIN: {X_train}')\n",
        "print(f'Y TRAIN: {Y_train}')\n",
        "print(f'X TEST: {X_test}')\n",
        "print(f'Y TEST: {Y_test}')\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "JeEIG4HwHMW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "ICYMV4HBI7nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    # Track correct predictions and total samples\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Iterate over training data\n",
        "    for inputs, labels in train_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = correct_train / total_train\n",
        "    print(f'Training Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Dictionary to store statistics for each artist\n",
        "    artist_stats = {artist: {'total': 0, 'correct': 0, 'most_predicted': None, 'accuracy': None} for artist in label_encoder.classes_}\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        if predicted[i] == Y_test[i]:\n",
        "            artist = label_encoder.classes_[Y_test[i]]\n",
        "            correct += 1\n",
        "            artist_stats[artist]['correct'] += 1\n",
        "        else:\n",
        "            artist = label_encoder.classes_[Y_test[i]]\n",
        "        artist_stats[artist]['total'] += 1\n",
        "\n",
        "        # Count most predicted artist\n",
        "        predicted_artist = label_encoder.classes_[predicted[i]]\n",
        "        if artist_stats[artist]['most_predicted'] is None:\n",
        "            artist_stats[artist]['most_predicted'] = {predicted_artist: 1}\n",
        "        elif predicted_artist not in artist_stats[artist]['most_predicted']:\n",
        "            artist_stats[artist]['most_predicted'][predicted_artist] = 1\n",
        "        else:\n",
        "            artist_stats[artist]['most_predicted'][predicted_artist] += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.4f}\\n')\n",
        "\n",
        "    # Print statistics for each artist\n",
        "    for artist, stats in artist_stats.items():\n",
        "        artist_accuracy = stats['correct'] / stats['total']\n",
        "        print(f'{artist} Accuracy: {artist_accuracy:.4f}')\n",
        "        if stats['most_predicted']:\n",
        "            most_predicted = max(stats['most_predicted'], key=stats['most_predicted'].get)\n",
        "            most_predicted_percentage = stats['most_predicted'][most_predicted] / stats['total'] * 100\n",
        "            print(f'Most predicted for {artist}: {most_predicted} ({most_predicted_percentage:.2f}%)\\n')\n",
        "        else:\n",
        "            print(f'No predictions for {artist}\\n')\n"
      ],
      "metadata": {
        "id": "cptOkG8sJX6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open('trainfile-2.json', 'r') as train_file:\n",
        "    train_data = json.load(train_file)\n",
        "\n",
        "with open('testfile-2.json', 'r') as test_file:\n",
        "    test_data = json.load(test_file)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_lines)\n",
        "\n",
        "print('\\n')\n",
        "train_sequences = tokenizer.texts_to_sequences(train_lines)\n",
        "print(train_sequences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_lines)\n",
        "print(test_sequences)\n",
        "\n",
        "print('\\n')\n",
        "max_seq_length = max([len(seq) for seq in train_sequences + test_sequences])\n",
        "print(max_seq_length)\n",
        "train_sequences_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "print(train_sequences_padded)\n",
        "test_sequences_padded = pad_sequences(test_sequences, maxlen=max_seq_length, padding='post')\n",
        "print(test_sequences_padded)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_authors)\n",
        "\n",
        "print('\\n')\n",
        "train_labels_encoded = label_encoder.transform(train_authors)\n",
        "print(train_labels_encoded)\n",
        "test_labels_encoded = label_encoder.transform(test_authors)\n",
        "print(test_labels_encoded)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_sequences_padded, train_labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'XTRAIN: {X_train}')\n",
        "print(f'XVAL: {X_val}')\n",
        "print(f'YTRAIN: {y_train}')\n",
        "print(f'YVAL: {y_val}')\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "input_size = len(tokenizer.word_index) + 1\n",
        "hidden_size = 128\n",
        "output_size = len(label_encoder.classes_)\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, output_size)\n",
        "\n",
        "c_loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "X_train_tensor = torch.LongTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_val_tensor = torch.LongTensor(X_val)\n",
        "y_val_tensor = torch.LongTensor(y_val)\n",
        "test_sequences_tensor = torch.LongTensor(test_sequences_padded)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(test_sequences_tensor)\n",
        "    test_outputs = model(test_sequences_tensor)\n",
        "    print(test_outputs)\n",
        "    predicted = torch.argmax(test_outputs, 1)\n",
        "    print(predicted)\n",
        "    correct = (predicted == torch.LongTensor(test_labels_encoded)).sum().item()\n",
        "    total = len(test_labels_encoded)\n",
        "    accuracy = correct / total\n",
        "    print(f'Overall Testing Accuracy: {accuracy:.4f}')\n",
        "    for i in range(len(test_lines)):\n",
        "        real_author = test_authors[i]\n",
        "        predicted_author = label_encoder.inverse_transform([predicted[i].item()])[0]\n",
        "        #print(f\"Line: {test_lines[i]}, Real Author: {real_author}, Predicted Author: {predicted_author}\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "YVB_Ixq3tFBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}